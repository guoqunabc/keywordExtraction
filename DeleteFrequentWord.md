## 常用词过滤方法

### 1. TF-IDF

1. TF(Term Frequency), IDF(Inverse Document Frequency)

2. Motivation：字词的重要性随着他在文件中出现的次数成正比增加，但同时随着他在语料库中出现的频率成反比下降。如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

3. TF表示词条在文档d中出现的频率。公示表示为: 
   $$
   对某一个词语t_i来说，其重要性：\\
   tf_{i, j} = \frac{n_{i, j}}{\sum_k n_{k, j}} \\
   n_{i, j}是该词在文件d_j中的出现次数，而分母则是在文件d_j中所有字词的出现次数之和
   $$

4. IDF的主要思想是：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。公示表示为：
   $$
   idf_i = log\frac{|D|}{1+|{j: t_i \in d_j}|} \\
   |D|:语料库中的文件数 \\
   分母：包含词语t_i的文件数目
   $$

5. 最后，
   $$
   tfidf_{i, j} = tf_{i, j} * idf_i
   $$


### 2. TF-IDF不足之处

1. 没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的
2. 按照传统TF-IDF函数标准，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。(换句话说，如果一个特征项只在某一个类别中的个别文本中大量出现，在类内的其他大部分文本中出现的很少，那么不排除这些个别文本是这个类中的特例情况，因此这样的特征项不具有代表性)。
3. 传统TF-IDF函数中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况
4. 对于文档中出现次数较少的重要人名、地名信息提取效果不佳

### 3. 改进方法

1. 改进TF：考虑将文档内的词频率更改为同一类文档内的词频率可以在一定程度上解决上面提到的第2项不足之处
2. 改进IDF：IDF=log(本类含特征词文档数m*总文档数N/所有含特征词文档数n+0.01)或者$IDF = log(1+\frac{P(m_k)}{P(m_k)'})$ . $P(m_k)$为特征词$m_k$在当前类别中的频率，$P(m_k)'$为其在其他类别中的频率。